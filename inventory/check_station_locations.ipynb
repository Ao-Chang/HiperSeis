{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station Inventory distance scale checking script\n",
    "\n",
    "This script is used to perform statistical checks on the station locations in the reconciled station metadata database against large catalog of seismic events with 'known' (i.e taken as 'ground truth') distances to stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of libraries and notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Add root of passive-seismic repository to Python path\n",
    "sys.path.append(os.path.realpath('..'))\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Customize matplotlib figure size for improve screen usage and resolution\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "# This is a bit dodgy since it assumes sperical earth...\n",
    "from obspy.geodetics.base import locations2degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar helper to indicate that slow tasks have not stalled\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardwired seed for repeatability\n",
    "import random\n",
    "random.seed(20190118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lightweight event catalog reader. TODO: Replace this with a shared, common event reader, e.g. see createEnsembleXML.py\n",
    "from dataio.catalogcsv import CatalogCSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure file locations for input and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for event catalog, whole folder read by CatalogCSV\n",
    "event_src_folder = r'/g/data/ha3/am7399/temp'\n",
    "# Input station inventory, generated by cleanup script engd2stxml.py\n",
    "inventory_src_file = r'/g/data/ha3/am7399/dev/passive-seismic/inventory/INVENTORY_20190116T144005.csv'\n",
    "# Path in which to save png plots of the distance correlation\n",
    "graphic_save_path = os.path.join(event_src_folder, 'plots')\n",
    "os.makedirs(graphic_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the event catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load event catalog. Need to use dill rather than pickle, to cover more data types.\n",
    "import dill as pkl\n",
    "pkl_file = os.path.join(event_src_folder, \"CAT.pkl\")\n",
    "if os.path.exists(pkl_file):\n",
    "    with open(pkl_file, 'rb') as f:\n",
    "        cat = pkl.load(f)\n",
    "else:\n",
    "    cat = CatalogCSV(event_src_folder)\n",
    "    with open(pkl_file, 'wb') as f:\n",
    "        pkl.dump(cat, f, pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load station inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv = pd.read_csv(inventory_src_file, sep=',', keep_default_na=False,\n",
    "                  parse_dates=['StationStart','StationEnd','ChannelStart','ChannelEnd'])\n",
    "# Only using BHZ channels\n",
    "inv = inv[inv['ChannelCode'] == 'BHZ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the event catalog against the station inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a main processing function\n",
    "\n",
    "This function loops through all the station codes in the catalog. Since most events do not have a network code, only a station code, we will gather all matching station codes from the station inventory to plot against the events.\n",
    "\n",
    "For each station in the event catalog, we compute the distance of all matching stations from the inventory and index them against the station code, with subindex of the full network.station code, collecting computed distances against reported distances for all 'P' events.\n",
    "\n",
    "For each station, we only gather up to MAX_EVENT_SAMPLES samples as this is plenty to perform a linear fit to the data. Note that when we extract the position of the stations from the inventory for a given event, we must consider the station and channel dates to make sure they contain the event date (+1 hour for the event to propagate to stations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCatalog(cat, inv, num_stations=-1, distfunc=locations2degrees):\n",
    "    '''Output has dictionary structure:\n",
    "            {\n",
    "             'StationCode':\n",
    "                {\n",
    "                 'NetworkCode.StationCode': \n",
    "                    {\n",
    "                     'Distance': [...],           # list of values\n",
    "                     'ComputedDistance': [...]    # list of values\n",
    "                    } \n",
    "                } \n",
    "            }\n",
    "    '''\n",
    "    hour = np.timedelta64(1, 'h')\n",
    "    #import pdb\n",
    "    MAX_EVENT_SAMPLES = 100\n",
    "    compute_dist = lambda x: distfunc(x.Latitude, x.Longitude, epicenter[0], epicenter[1])\n",
    "    stations = [s for s in cat.station_dict.keys()]\n",
    "    if num_stations > 0:\n",
    "        stations = stations[0:num_stations]\n",
    "    agg_result = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    progress = tqdm(total=len(stations))\n",
    "    # Precompute full names\n",
    "    inv['FullCode'] = inv['NetworkCode'].map(str) + \".\" + inv['StationCode']\n",
    "    for stn in stations:\n",
    "        progress.update()\n",
    "        # Only consider P waves\n",
    "        station_events = cat.station_dict[stn]['P']\n",
    "        if len(station_events) == 0:\n",
    "            continue\n",
    "        #pdb.set_trace()\n",
    "        df_st = inv[inv['StationCode'] == stn]\n",
    "        if df_st.empty:\n",
    "            continue\n",
    "        # Precompute some masks\n",
    "        stst = df_st['StationStart']\n",
    "        sten = df_st['StationEnd']\n",
    "        chst = df_st['ChannelStart']\n",
    "        chen = df_st['ChannelEnd']\n",
    "        stst_na = stst.isna().values\n",
    "        sten_na = sten.isna().values\n",
    "        chst_na = chst.isna().values\n",
    "        chen_na = chen.isna().values\n",
    "        # Don't need to plot all events to detect distance error, a sampling of events will do.\n",
    "        num_samples = min(MAX_EVENT_SAMPLES, len(station_events))\n",
    "        for event_id, event_distance_deg in random.sample(station_events, num_samples):\n",
    "            event = cat.event_dict[event_id]\n",
    "            epicenter = event.preferred_origin.epicenter()\n",
    "            event_date = np.datetime64(event.preferred_origin.utctime)\n",
    "            event_window_end = event_date + hour\n",
    "            station_mask = stst_na | ((stst.values < event_date) & (sten_na | (sten.values > event_window_end)))\n",
    "            channel_mask = chst_na | ((chst.values < event_date) & (chen_na | (chen.values > event_window_end)))\n",
    "            df_event = df_st[station_mask & channel_mask]\n",
    "            if df_event.empty:\n",
    "                continue\n",
    "            computed_dist_deg = df_event.apply(compute_dist, axis=1)\n",
    "            for idx, dist in computed_dist_deg.iteritems():\n",
    "                full_code = df_event.loc[idx, \"FullCode\"]\n",
    "                agg_result[stn][full_code]['Distance'].append(event_distance_deg)\n",
    "                agg_result[stn][full_code]['ComputedDistance'].append(dist)\n",
    "    progress.close()\n",
    "    return agg_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the main distance processing function. Will load cached pickled result if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_profile = False\n",
    "if do_profile:\n",
    "    %lprun -f processCatalog agg_result=processCatalog(cat, inv, 100)\n",
    "else:\n",
    "    import dill as pkl\n",
    "    pkl_file = os.path.join(event_src_folder, \"distances.pkl\")\n",
    "    if os.path.exists(pkl_file):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            agg_result = pkl.load(f)\n",
    "    else:\n",
    "        # Perform processing of distances\n",
    "        agg_result = processCatalog(cat, inv)\n",
    "        with open(pkl_file, 'wb') as f:\n",
    "            pkl.dump(agg_result, f, pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting and analyses\n",
    "\n",
    "For each station code, we generate an x-y scatter plot of event reported distance (ground truth) against computed distance. Where multiple networks match the station code, we overly the plots for each network in order to ascertain whether they refer to the same station or a geographically distinct station. Note: due to number of stations, the plotting loop is expensive, so is disabled by default in this notebook.\n",
    "\n",
    "The results dictionary from the above _processCatalog()_ function is flattened to be indexed by NetworkCode.StationCode, and then for each we perform a simple least squares linear regression to determine the slope and y-intercept of the Distance vs ComputedDistance, as well as computing an RMS error and a count of the number of contributing seismic events. These are then used to filter the results, looking for outliers with poor correlation between expected and actual distance.\n",
    "\n",
    "Once the filters are applied to detect candidate bad stations, the candidate list is written to file, and the corresponding distance plots for those stations are collected for review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all distance plots (disabled by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotStationDistances(stn, stn_data, savepath=None):\n",
    "    '''Plotting function for the data for a single station code as generated by function processCatalog().\n",
    "    '''\n",
    "    import itertools\n",
    "    import os\n",
    "    marker = itertools.cycle(('x', '1', '+', '2'))\n",
    "    xval = [v['Distance'] for v in stn_data.values()]\n",
    "    yval = [v['ComputedDistance'] for v in stn_data.values()]\n",
    "    # Overlay plot for each network code\n",
    "    [plt.plot(x, y, marker=next(marker), markersize=10, linestyle='', fillstyle=None) for x, y in zip(xval, yval)]\n",
    "    plt.axis('equal')\n",
    "    plt.title(stn, fontsize=16)\n",
    "    plt.legend(stn_data.keys(), fontsize=14)\n",
    "    plt.grid()\n",
    "    plt.gca().tick_params(axis='both', labelsize=14)\n",
    "    plt.xlabel('Distance (deg)', fontsize=14)\n",
    "    plt.ylabel('Computed Distance (deg)', fontsize=14)    \n",
    "    if savepath:\n",
    "        plt.savefig(os.path.join(savepath, stn + \".png\"), dpi=100)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dump images to files. Takes ~ 1 hour, so only run when condition set to True.\n",
    "if False:\n",
    "    [plotStationDistances(stn, stn_data, graphic_save_path) for stn, stn_data in agg_result.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peform statistical analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, flatten the result dictionary to eliminate the top level redundant key.\n",
    "stn_data = {k: v for stn, netstn in agg_result.items() for k, v in netstn.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each network.station, compute a linear regression to the distance data points.\n",
    "# Each regression will generate a 2D point (y-intercept, slope).\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "progress = tqdm(len(stn_data))\n",
    "# Collect the results in a dict by network.station code. Each entry contains regression results, RMSE and event count.\n",
    "lr_dict = {}\n",
    "for k, v in stn_data.items():\n",
    "    progress.update()\n",
    "    lm = linear_model.LinearRegression()\n",
    "    xx = np.array(v['Distance'])\n",
    "    yy = np.array(v['ComputedDistance'])\n",
    "    rmse = np.sqrt(mean_squared_error(xx, yy))\n",
    "    event_count = len(xx)\n",
    "    model = lm.fit(np.reshape(xx, (-1, 1)), np.reshape(yy, (-1, 1)))\n",
    "    # Store as tuple of ('Intercept', 'Slope', 'RMSE', 'EventCount')\n",
    "    lr_dict[k] = (*model.intercept_, *model.coef_.flatten(), rmse, event_count)\n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the dict into a Pandas dataframe,, indexed by station identity\n",
    "lr_df = pd.DataFrame.from_dict(lr_dict, orient='index', columns=['Intercept', 'Slope', 'RMSE', 'EventCount'])\n",
    "lr_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the tabulated regression and error data to identify poor distance correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn for 2D distribution plots\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alias data series to friendlier names\n",
    "slope = lr_df['Slope']\n",
    "intercept = lr_df['Intercept']\n",
    "rmse = lr_df['RMSE']\n",
    "counts = lr_df['EventCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of slope values (optimal is 1.0)\n",
    "min(slope), max(slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of intercept values (optimal is 0.0)\n",
    "min(intercept), max(intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of RMSE values (optimal is 0.0)\n",
    "min(rmse), max(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of count values (ignore bad fit when counts < 5)\n",
    "min(counts), max(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE HISTOGRAMS TO VIEW DISTRIBUTION OF THE COLLECTED STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(slope, 100, range=(0.98, 1.02))\n",
    "plt.title('Slope distribution near 1')\n",
    "plt.xlabel('Slope')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(intercept, 100, range=(-1, 1))\n",
    "plt.title('Intercept distribution about 0')\n",
    "plt.xlabel('y-intercept (deg)')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(rmse, 100, range=(0, 1))\n",
    "plt.title('RMSE distribution')\n",
    "plt.xlabel('RMSE (deg)')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(counts, 100)\n",
    "plt.title('Event count distribution')\n",
    "plt.xlabel('Num. events')\n",
    "plt.ylabel('Bin count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 2D KDE plot to visualize distribution of slope and intercept near optimal point\n",
    "\n",
    "We mask the data to the vicinity of the optimal point so that the detail can be seen. The outliers are so far away that if we include them, we can't see any detail near the optimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked = ((slope>=0.99) & (slope<=1.01) & (intercept>=-0.5) & (intercept<=0.5))\n",
    "sns.kdeplot(slope[masked], intercept[masked], shade=True, color='C0').set_title(\"Distribution of near-optimal linear fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat previous plot, but with a wider range of values and masked by low RMSE for an alternate view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked = (rmse <= 0.5)\n",
    "p = sns.kdeplot(slope[masked], rmse[masked], shade=True, color='C0', gridsize=100).set(xlim=(0.9, 1.1), ylim=(-5, 5))\n",
    "#p.set_title(\"Distribution of low RMSE linear fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the above visualizations, a final composite mask is applied to identify BAD distance data\n",
    "\n",
    "The code here self-documents the limits applied. A station whose distance data is considerd 'bad' must satisfy the following criteria:\n",
    "* either slope or intercept must be outside reasonable range, AND\n",
    "* RMSE must be outside reasonable range, AND\n",
    "* the number of seismic events used for the statistics must be at least MIN_EVENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_EVENTS = 5\n",
    "slope_lims = (0.95, 1.05)\n",
    "intercept_lims = (-1.0, 1.0)\n",
    "rmse_lim = 0.5\n",
    "\n",
    "s0 = (slopes < slope_lims[0])\n",
    "s1 = (slopes > slope_lims[1])\n",
    "i0 = (intercepts < intercept_lims[0])\n",
    "i1 = (intercepts > intercept_lims[1])\n",
    "ec_mask = (counts >= MIN_EVENTS)\n",
    "rmse_mask = (rmse > rmse_lim)\n",
    "\n",
    "# Display number of stations captured by each mask individually\n",
    "print(s0.sum(), s1.sum(), i0.sum(), i1.sum(), ec_mask.sum(), rmse_mask.sum())\n",
    "\n",
    "# Compute the aggregate mask for identifying 'bad' stations\n",
    "outliers_mask = (s0 | s1 | i0 | i1) & ec_mask & rmse_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the Pandas DataFrame to the bad stations and emit one line report\n",
    "df_outliers = lr_df[outliers_mask]\n",
    "bad_stn_codes = [s.split('.')[1] for s in df_outliers.index.values]\n",
    "print(len(df_outliers), '/', len(lr_df), ' stations identified as bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emit results to file in tabulated form, and collect the corresponding plots for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_STATIONS_FILE_NAME = \"bad_distance_scale.txt\"\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    with open(os.path.join(graphic_save_path, BAD_STATIONS_FILE_NAME), \"w\") as f:\n",
    "        s = str(df_outliers)\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "bad_plots_path = os.path.join(event_src_folder, 'bad_plots')\n",
    "os.makedirs(bad_plots_path, exist_ok=True)\n",
    "for bad_stn in bad_stn_codes:\n",
    "    bad_stn_plotname = bad_stn + \".png\"\n",
    "    src_file = os.path.join(graphic_save_path, bad_stn_plotname)\n",
    "    dst_file = os.path.join(bad_plots_path, bad_stn_plotname)\n",
    "    shutil.copy2(src_file, dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
